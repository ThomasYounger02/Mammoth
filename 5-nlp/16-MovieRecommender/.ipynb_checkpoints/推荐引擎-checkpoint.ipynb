{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e083d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tests as t\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00eb615b",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d669bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.shape[0]\n",
    "\n",
    "reviews.user_id.nunique()\n",
    "\n",
    "reviews.rating.isnull().mean()\n",
    "\n",
    "\n",
    "# number of different genres\n",
    "genres = []\n",
    "for val in movies.genre:\n",
    "    try:\n",
    "        genres.extend(val.split('|'))\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "# we end up needing this later\n",
    "genres = set(genres)\n",
    "print(\"The number of genres is {}.\".format(len(genres)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee2d5d9",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19326fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull date if it exists\n",
    "create_date = lambda val: val[-5:-1] if val[-1] == ')' else np.nan\n",
    "\n",
    "# apply the function to pull the date\n",
    "movies['date'] = movies['movie'].apply(create_date)\n",
    "\n",
    "# Return century of movie as a dummy column\n",
    "def add_movie_year(val):\n",
    "    if val[:2] == yr:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "        \n",
    "# Apply function\n",
    "for yr in ['18', '19', '20']:\n",
    "    movies[str(yr) + \"00's\"] = movies['date'].apply(add_movie_year)\n",
    "\n",
    "\n",
    "\n",
    "# Function to split and return values for columns\n",
    "def split_genres(val):\n",
    "    try:\n",
    "        if val.find(gene) >-1:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    except AttributeError:\n",
    "        return 0\n",
    "\n",
    "# Apply function for each genre\n",
    "for gene in genres:        \n",
    "    movies[gene] = movies['genre'].apply(split_genres)\n",
    "\n",
    "\n",
    "import datetime\n",
    "\n",
    "change_timestamp = lambda val: datetime.datetime.fromtimestamp(int(val)).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "reviews['date'] = reviews['timestamp'].apply(change_timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4876ed20",
   "metadata": {},
   "source": [
    "# Knowledge Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4f0748",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tests as t\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7bd67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How To Find The Most Popular Movies?\n",
    "def create_ranked_df(movies, reviews):\n",
    "        '''\n",
    "        INPUT\n",
    "        movies - the movies dataframe\n",
    "        reviews - the reviews dataframe\n",
    "        \n",
    "        OUTPUT\n",
    "        ranked_movies - a dataframe with movies that are sorted by highest avg rating, more reviews, \n",
    "                        then time, and must have more than 4 ratings\n",
    "        '''\n",
    "        \n",
    "        # Pull the average ratings and number of ratings for each movie\n",
    "        movie_ratings = reviews.groupby('movie_id')['rating']\n",
    "        avg_ratings = movie_ratings.mean()\n",
    "        num_ratings = movie_ratings.count()\n",
    "        last_rating = pd.DataFrame(reviews.groupby('movie_id').max()['date'])\n",
    "        last_rating.columns = ['last_rating']\n",
    "\n",
    "        # Add Dates\n",
    "        rating_count_df = pd.DataFrame({'avg_rating': avg_ratings, 'num_ratings': num_ratings})\n",
    "        rating_count_df = rating_count_df.join(last_rating)\n",
    "\n",
    "        # merge with the movies dataset\n",
    "        movie_recs = movies.set_index('movie_id').join(rating_count_df)\n",
    "\n",
    "        # sort by top avg rating and number of ratings\n",
    "        ranked_movies = movie_recs.sort_values(['avg_rating', 'num_ratings', 'last_rating'], ascending=False)\n",
    "\n",
    "        # for edge cases - subset the movie list to those with only 5 or more reviews\n",
    "        ranked_movies = ranked_movies[ranked_movies['num_ratings'] > 4]\n",
    "        \n",
    "        return ranked_movies\n",
    "    \n",
    "\n",
    "def popular_recommendations(user_id, n_top, ranked_movies):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - the user_id (str) of the individual you are making recommendations for\n",
    "    n_top - an integer of the number recommendations you want back\n",
    "    ranked_movies - a pandas dataframe of the already ranked movies based on avg rating, count, and time\n",
    "\n",
    "    OUTPUT:\n",
    "    top_movies - a list of the n_top recommended movies by movie title in order best to worst\n",
    "    '''\n",
    "\n",
    "    top_movies = list(ranked_movies['movie'][:n_top])\n",
    "\n",
    "    return top_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a53616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Filters\n",
    "def popular_recs_filtered(user_id, n_top, ranked_movies, years=None, genres=None):\n",
    "    '''\n",
    "    REDO THIS DOC STRING\n",
    "    \n",
    "    INPUT:\n",
    "    user_id - the user_id (str) of the individual you are making recommendations for\n",
    "    n_top - an integer of the number recommendations you want back\n",
    "    ranked_movies - a pandas dataframe of the already ranked movies based on avg rating, count, and time\n",
    "    years - a list of strings with years of movies\n",
    "    genres - a list of strings with genres of movies\n",
    "    \n",
    "    OUTPUT:\n",
    "    top_movies - a list of the n_top recommended movies by movie title in order best to worst\n",
    "    '''\n",
    "    # Filter movies based on year and genre\n",
    "    if years is not None:\n",
    "        ranked_movies = ranked_movies[ranked_movies['date'].isin(years)]\n",
    "\n",
    "    if genres is not None:\n",
    "        num_genre_match = ranked_movies[genres].sum(axis=1)\n",
    "        ranked_movies = ranked_movies.loc[num_genre_match > 0, :]\n",
    "            \n",
    "            \n",
    "    # create top movies list \n",
    "    top_movies = list(ranked_movies['movie'][:n_top])\n",
    "\n",
    "    return top_movies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79268f1a",
   "metadata": {},
   "source": [
    "# Measuring Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6ebd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr, kendalltau\n",
    "import matplotlib.pyplot as plt\n",
    "import tests as t\n",
    "import helper as h\n",
    "%matplotlib inline\n",
    "\n",
    "play_data = pd.DataFrame({'x1': [-3, -2, -1, 0, 1, 2, 3], \n",
    "               'x2': [9, 4, 1, 0, 1, 4, 9],\n",
    "               'x3': [1, 2, 3, 4, 5, 6, 7],\n",
    "               'x4': [2, 5, 15, 27, 28, 30, 31]\n",
    "})\n",
    "\n",
    "#create play data dataframe\n",
    "play_data = play_data[['x1', 'x2', 'x3', 'x4']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8151da68",
   "metadata": {},
   "source": [
    "## Pearson's Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce17725a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_corr(x, y):\n",
    "    '''\n",
    "    INPUT\n",
    "    x - an array of matching length to array y\n",
    "    y - an array of matching length to array x\n",
    "    OUTPUT\n",
    "    corr - the pearson correlation coefficient for comparing x and y\n",
    "    '''\n",
    "    # Compute Mean Values\n",
    "    mean_x, mean_y = np.sum(x)/len(x), np.sum(y)/len(y) \n",
    "    \n",
    "    x_diffs = x - mean_x\n",
    "    y_diffs = y - mean_y\n",
    "    numerator = np.sum(x_diffs*y_diffs)\n",
    "    denominator = np.sqrt(np.sum(x_diffs**2))*np.sqrt(np.sum(y_diffs**2))\n",
    "        \n",
    "    corr = numerator/denominator\n",
    "                            \n",
    "    return corr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4807b8e6",
   "metadata": {},
   "source": [
    "## Spearman's Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e539782",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_spearman(x, y):\n",
    "    '''\n",
    "    INPUT\n",
    "    x - an array of matching length to array y\n",
    "    y - an array of matching length to array x\n",
    "    OUTPUT\n",
    "    corr - the spearman correlation coefficient for comparing x and y\n",
    "    '''\n",
    "    # Change each vector to ranked values\n",
    "    x = x.rank()\n",
    "    y = y.rank()\n",
    "    \n",
    "    # Compute Mean Values\n",
    "    mean_x, mean_y = np.sum(x)/len(x), np.sum(y)/len(y) \n",
    "    \n",
    "    x_diffs = x - mean_x\n",
    "    y_diffs = y - mean_y\n",
    "    numerator = np.sum(x_diffs*y_diffs)\n",
    "    denominator = np.sqrt(np.sum(x_diffs**2))*np.sqrt(np.sum(y_diffs**2))\n",
    "        \n",
    "    corr = numerator/denominator\n",
    "                            \n",
    "    return corr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611c9a96",
   "metadata": {},
   "source": [
    "## Kendall's Tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3b6cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kendalls_tau(x, y):\n",
    "    '''\n",
    "    INPUT\n",
    "    x - an array of matching length to array y\n",
    "    y - an array of matching length to array x\n",
    "    OUTPUT\n",
    "    tau - the kendall's tau for comparing x and y\n",
    "    '''    \n",
    "    # Change each vector to ranked values\n",
    "    x = x.rank()\n",
    "    y = y.rank()\n",
    "    n = len(x)\n",
    "     \n",
    "    sum_vals = 0\n",
    "    # Compute Mean Values\n",
    "    for i, (x_i, y_i) in enumerate(zip(x, y)):\n",
    "        for j, (x_j, y_j) in enumerate(zip(x, y)):\n",
    "            if i < j:\n",
    "                sum_vals += np.sign(x_i - x_j)*np.sign(y_i - y_j)\n",
    "                        \n",
    "    tau = 2*sum_vals/(n*(n-1))\n",
    "    \n",
    "    return tau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea82e208",
   "metadata": {},
   "source": [
    "## Euclidean Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e290b164",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eucl_dist(x, y):\n",
    "    '''\n",
    "    INPUT\n",
    "    x - an array of matching length to array y\n",
    "    y - an array of matching length to array x\n",
    "    OUTPUT\n",
    "    euc - the euclidean distance between x and y\n",
    "    '''  \n",
    "    return np.linalg.norm(x - y)\n",
    "    \n",
    "def manhat_dist(x, y):\n",
    "    '''\n",
    "    INPUT\n",
    "    x - an array of matching length to array y\n",
    "    y - an array of matching length to array x\n",
    "    OUTPUT\n",
    "    manhat - the manhattan distance between x and y\n",
    "    '''  \n",
    "    return sum(abs(e - s) for s, e in zip(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c1fecb",
   "metadata": {},
   "source": [
    "# Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3a1491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tests as t\n",
    "from scipy.sparse import csr_matrix\n",
    "from IPython.display import HTML\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Read in the datasets\n",
    "movies = pd.read_csv('movies_clean.csv')\n",
    "reviews = pd.read_csv('reviews_clean.csv')\n",
    "\n",
    "del movies['Unnamed: 0']\n",
    "del reviews['Unnamed: 0']\n",
    "\n",
    "print(reviews.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6288e0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_items = reviews[['user_id', 'movie_id', 'rating']]\n",
    "user_items.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca3986c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create user-by-item matrix\n",
    "user_by_movie = user_items.groupby(['user_id', 'movie_id'])['rating'].max().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a12288",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert movies.shape[0] == user_by_movie.shape[1], \"Oh no! Your matrix should have {} columns, and yours has {}!\".format(movies.shape[0], user_by_movie.shape[1])\n",
    "assert reviews.user_id.nunique() == user_by_movie.shape[0], \"Oh no! Your matrix should have {} rows, and yours has {}!\".format(reviews.user_id.nunique(), user_by_movie.shape[0])\n",
    "print(\"Looks like you are all set! Proceed!\")\n",
    "HTML('<img src=\"images/greatjob.webp\">')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a790a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary with users and corresponding movies seen\n",
    "\n",
    "def movies_watched(user_id):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - the user_id of an individual as int\n",
    "    OUTPUT:\n",
    "    movies - an array of movies the user has watched\n",
    "    '''\n",
    "    movies = user_by_movie.loc[user_id][user_by_movie.loc[user_id].isnull() == False].index.values\n",
    "\n",
    "    return movies\n",
    "\n",
    "\n",
    "def create_user_movie_dict():\n",
    "    '''\n",
    "    INPUT: None\n",
    "    OUTPUT: movies_seen - a dictionary where each key is a user_id and the value is an array of movie_ids\n",
    "    \n",
    "    Creates the movies_seen dictionary\n",
    "    '''\n",
    "    n_users = user_by_movie.shape[0]\n",
    "    movies_seen = dict()\n",
    "\n",
    "    for user1 in range(1, n_users+1):\n",
    "        \n",
    "        # assign list of movies to each user key\n",
    "        movies_seen[user1] = movies_watched(user1)\n",
    "    \n",
    "    return movies_seen\n",
    "    \n",
    "movies_seen = create_user_movie_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951f15a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove individuals who have watched 2 or fewer movies - don't have enough data to make recs\n",
    "\n",
    "def create_movies_to_analyze(movies_seen, lower_bound=2):\n",
    "    '''\n",
    "    INPUT:  \n",
    "    movies_seen - a dictionary where each key is a user_id and the value is an array of movie_ids\n",
    "    lower_bound - (an int) a user must have more movies seen than the lower bound to be added to the movies_to_analyze dictionary\n",
    "\n",
    "    OUTPUT: \n",
    "    movies_to_analyze - a dictionary where each key is a user_id and the value is an array of movie_ids\n",
    "    \n",
    "    The movies_seen and movies_to_analyze dictionaries should be the same except that the output dictionary has removed \n",
    "    \n",
    "    '''\n",
    "    movies_to_analyze = dict()\n",
    "\n",
    "    for user, movies in movies_seen.items():\n",
    "        if len(movies) > lower_bound:\n",
    "            movies_to_analyze[user] = movies\n",
    "    return movies_to_analyze\n",
    "\n",
    "movies_to_analyze = create_movies_to_analyze(movies_seen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a65b444",
   "metadata": {},
   "source": [
    "# Calculating User Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f58f36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_correlation(user1, user2):\n",
    "    '''\n",
    "    INPUT\n",
    "    user1 - int user_id\n",
    "    user2 - int user_id\n",
    "    OUTPUT\n",
    "    the correlation between the matching ratings between the two users\n",
    "    '''\n",
    "    # Pull movies for each user\n",
    "    movies1 = movies_to_analyze[user1]\n",
    "    movies2 = movies_to_analyze[user2]\n",
    "    \n",
    "    \n",
    "    # Find Similar Movies\n",
    "    sim_movs = np.intersect1d(movies1, movies2, assume_unique=True)\n",
    "    \n",
    "    # Calculate correlation between the users\n",
    "    df = user_by_movie.loc[(user1, user2), sim_movs]\n",
    "    corr = df.transpose().corr().iloc[0,1]\n",
    "    \n",
    "    return corr #return the correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f70fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which movies did both user 2 and user 104 see?\n",
    "set_2 = set(movies_to_analyze[2])\n",
    "set_104 = set(movies_to_analyze[104])\n",
    "set_2.intersection(set_104)\n",
    "\n",
    "# What were the ratings for each user on those movies?\n",
    "print(user_by_movie.loc[2, set_2.intersection(set_104)])\n",
    "print(user_by_movie.loc[104, set_2.intersection(set_104)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ddb361",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_euclidean_dist(user1, user2):\n",
    "    '''\n",
    "    INPUT\n",
    "    user1 - int user_id\n",
    "    user2 - int user_id\n",
    "    OUTPUT\n",
    "    the euclidean distance between user1 and user2\n",
    "    '''\n",
    "    # Pull movies for each user\n",
    "    movies1 = movies_to_analyze[user1]\n",
    "    movies2 = movies_to_analyze[user2]\n",
    "    \n",
    "    \n",
    "    # Find Similar Movies\n",
    "    sim_movs = np.intersect1d(movies1, movies2, assume_unique=True)\n",
    "    \n",
    "    # Calculate euclidean distance between the users\n",
    "    df = user_by_movie.loc[(user1, user2), sim_movs]\n",
    "    dist = np.linalg.norm(df.loc[user1] - df.loc[user2])\n",
    "    \n",
    "    return dist #return the euclidean distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8b2cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read in solution euclidean distances\"\n",
    "import pickle\n",
    "df_dists = pd.read_pickle(\"data/Term2/recommendations/lesson1/data/dists.p\")\n",
    "\n",
    "\n",
    "# Test your function against the solution\n",
    "assert compute_euclidean_dist(2,2) == df_dists.query(\"user1 == 2 and user2 == 2\")['eucl_dist'][0], \"Oops!  The distance between a user and itself should be 0.0.\"\n",
    "assert round(compute_euclidean_dist(2,66), 2) == round(df_dists.query(\"user1 == 2 and user2 == 66\")['eucl_dist'][1], 2), \"Oops!  The distance between user 2 and 66 should be about 2.24.\"\n",
    "assert np.isnan(compute_euclidean_dist(2,104)) == np.isnan(df_dists.query(\"user1 == 2 and user2 == 104\")['eucl_dist'][4]), \"Oops!  The distance between user 2 and 104 should be 2.\"\n",
    "\n",
    "print(\"If this is all you see, then it looks like your function passed all of our tests!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a597510",
   "metadata": {},
   "source": [
    "# Using the Nearest Neighbors to Make Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cf3d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_neighbors(user):\n",
    "    '''\n",
    "    INPUT:\n",
    "        user - (int) the user_id of the individual you want to find the closest users\n",
    "    OUTPUT:\n",
    "        closest_neighbors - an array of the id's of the users sorted from closest to farthest away\n",
    "    '''\n",
    "    # I treated ties as arbitrary and just kept whichever was easiest to keep using the head method\n",
    "    # You might choose to do something less hand wavy\n",
    "    \n",
    "    closest_users = df_dists[df_dists['user1']==user].sort_values(by='eucl_dist').iloc[1:]['user2']\n",
    "    closest_neighbors = np.array(closest_users)\n",
    "    \n",
    "    return closest_neighbors\n",
    "    \n",
    "    \n",
    "    \n",
    "def movies_liked(user_id, min_rating=7):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - the user_id of an individual as int\n",
    "    min_rating - the minimum rating considered while still a movie is still a \"like\" and not a \"dislike\"\n",
    "    OUTPUT:\n",
    "    movies_liked - an array of movies the user has watched and liked\n",
    "    '''\n",
    "    movies_liked = np.array(user_items.query('user_id == @user_id and rating > (@min_rating -1)')['movie_id'])\n",
    "    \n",
    "    return movies_liked\n",
    "\n",
    "\n",
    "def movie_names(movie_ids):\n",
    "    '''\n",
    "    INPUT\n",
    "    movie_ids - a list of movie_ids\n",
    "    OUTPUT\n",
    "    movies - a list of movie names associated with the movie_ids\n",
    "    \n",
    "    '''\n",
    "    movie_lst = list(movies[movies['movie_id'].isin(movie_ids)]['movie'])\n",
    "   \n",
    "    return movie_lst\n",
    "    \n",
    "    \n",
    "def make_recommendations(user, num_recs=10):\n",
    "    '''\n",
    "    INPUT:\n",
    "        user - (int) a user_id of the individual you want to make recommendations for\n",
    "        num_recs - (int) number of movies to return\n",
    "    OUTPUT:\n",
    "        recommendations - a list of movies - if there are \"num_recs\" recommendations return this many\n",
    "                          otherwise return the total number of recommendations available for the \"user\"\n",
    "                          which may just be an empty list\n",
    "    '''\n",
    "    # I wanted to make recommendations by pulling different movies than the user has already seen\n",
    "    # Go in order from closest to farthest to find movies you would recommend\n",
    "    # I also only considered movies where the closest user rated the movie as a 9 or 10\n",
    "    \n",
    "    # movies_seen by user (we don't want to recommend these)\n",
    "    movies_seen = movies_watched(user)\n",
    "    closest_neighbors = find_closest_neighbors(user)\n",
    "    \n",
    "    # Keep the recommended movies here\n",
    "    recs = np.array([])\n",
    "    \n",
    "    # Go through the neighbors and identify movies they like the user hasn't seen\n",
    "    for neighbor in closest_neighbors:\n",
    "        neighbs_likes = movies_liked(neighbor)\n",
    "        \n",
    "        #Obtain recommendations for each neighbor\n",
    "        new_recs = np.setdiff1d(neighbs_likes, movies_seen, assume_unique=True)\n",
    "        \n",
    "        # Update recs with new recs\n",
    "        recs = np.unique(np.concatenate([new_recs, recs], axis=0))\n",
    "        \n",
    "        # If we have enough recommendations exit the loop\n",
    "        if len(recs) > num_recs-1:\n",
    "            break\n",
    "    \n",
    "    # Pull movie titles using movie ids\n",
    "    recommendations = movie_names(recs)\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "def all_recommendations(num_recs=10):\n",
    "    '''\n",
    "    INPUT \n",
    "        num_recs (int) the (max) number of recommendations for each user\n",
    "    OUTPUT\n",
    "        all_recs - a dictionary where each key is a user_id and the value is an array of recommended movie titles\n",
    "    '''\n",
    "    \n",
    "    # All the users we need to make recommendations for\n",
    "    users = np.unique(df_dists['user1'])\n",
    "    n_users = len(users)\n",
    "    \n",
    "    #Store all recommendations in this dictionary\n",
    "    all_recs = dict()\n",
    "    \n",
    "    # Make the recommendations for each user\n",
    "    for user in users:\n",
    "        all_recs[user] = make_recommendations(user, num_recs)\n",
    "    \n",
    "    return all_recs\n",
    "\n",
    "all_recs = all_recommendations(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfae6ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loads our solution dictionary so you can compare results - FULL PATH IS \"data/Term2/recommendations/lesson1/data/all_recs.p\"\n",
    "all_recs_sol = pd.read_pickle(\"data/Term2/recommendations/lesson1/data/all_recs.p\")\n",
    "\n",
    "\n",
    "assert all_recs[2] == make_recommendations(2), \"Oops!  Your recommendations for user 2 didn't match ours.\"\n",
    "assert all_recs[26] == make_recommendations(26), \"Oops!  It actually wasn't possible to make any recommendations for user 26.\"\n",
    "assert all_recs[1503] == make_recommendations(1503), \"Oops! Looks like your solution for user 1503 didn't match ours.\"\n",
    "print(\"If you made it here, you now have recommendations for many users using collaborative filtering!\")\n",
    "HTML('<img src=\"images/greatjob.webp\">')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf8f252",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Users without recs\n",
    "users_without_recs = []\n",
    "for user, movie_recs in all_recs.items():\n",
    "    if len(movie_recs) == 0:\n",
    "        users_without_recs.append(user)\n",
    "    \n",
    "len(users_without_recs)\n",
    "\n",
    "\n",
    "# NaN euclidean distance values\n",
    "df_dists['eucl_dist'].isnull().sum()\n",
    "\n",
    "# Users with fewer than 10 recs\n",
    "users_with_less_than_10recs = []\n",
    "for user, movie_recs in all_recs.items():\n",
    "    if len(movie_recs) < 10:\n",
    "        users_with_less_than_10recs.append(user)\n",
    "    \n",
    "len(users_with_less_than_10recs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd6e2e5",
   "metadata": {},
   "source": [
    "## Content Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8515d841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from IPython.display import HTML\n",
    "import progressbar\n",
    "import tests as t\n",
    "import pickle\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Read in the datasets\n",
    "movies = pd.read_csv('movies_clean.csv')\n",
    "reviews = pd.read_csv('reviews_clean.csv')\n",
    "\n",
    "del movies['Unnamed: 0']\n",
    "del reviews['Unnamed: 0']\n",
    "\n",
    "\n",
    "all_recs = pickle.load(open(\"all_recs.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e41820",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_with_all_recs = []\n",
    "for user, movie_recs in all_recs.items():\n",
    "    if len(movie_recs) > 9:\n",
    "        users_with_all_recs.append(user)\n",
    "\n",
    "print(\"There are {} users with all reccomendations from collaborative filtering.\".format(len(users_with_all_recs)))\n",
    "\n",
    "users = np.unique(reviews['user_id'])\n",
    "users_who_need_recs = np.setdiff1d(users, users_with_all_recs)\n",
    "\n",
    "print(\"There are {} users who still need recommendations.\".format(len(users_who_need_recs)))\n",
    "print(\"This means that only {}% of users received all 10 of their recommendations using collaborative filtering\".format(round(len(users_with_all_recs)/len(np.unique(reviews['user_id'])), 4)*100))\n",
    "\n",
    "# Some test here might be nice\n",
    "assert len(users_with_all_recs) == 22187\n",
    "print(\"That's right there were still another 31781 users who needed recommendations when we only used collaborative filtering!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431c9ce6",
   "metadata": {},
   "source": [
    "### Content Based Recommendations\n",
    "\n",
    "Before finding recommendations, rank the user's ratings from highest to lowest. You will move through the movies in this order looking for other similar movies.\n",
    "\n",
    "```python\n",
    "# create a dataframe similar to reviews, but ranked by rating for each user\n",
    "ranked_reviews = reviews.sort_values(by=['user_id', 'rating'], ascending=False)\n",
    "```\n",
    "\n",
    "Create a numpy array that is a matrix of indicator variables related to year (by century) and movie genres by movie. Perform the dot product of this matrix with itself (transposed) to obtain a similarity matrix of each movie with every other movie. The final matrix should be 31245 x 31245.\n",
    "\n",
    "```python\n",
    "\n",
    "# Subset so movie_content is only using the dummy variables for each genre and the 3 century based year dummy columns\n",
    "movie_content = np.array(movies.iloc[:,4:])\n",
    "\n",
    "# Take the dot product to obtain a movie x movie matrix of similarities\n",
    "dot_prod_movies = movie_content.dot(np.transpose(movie_content))\n",
    "\n",
    "# create checks for the dot product matrix\n",
    "assert dot_prod_movies.shape[0] == 31245\n",
    "assert dot_prod_movies.shape[1] == 31245\n",
    "assert dot_prod_movies[0, 0] == np.max(dot_prod_movies[0])\n",
    "print(\"Looks like you passed all of the tests.  Though they weren't very robust - if you want to write some of your own, I won't complain!\")\n",
    "```\n",
    "\n",
    "In the below cell, complete each of the functions needed for making content based recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3fc6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_movies(movie_id):\n",
    "    '''\n",
    "    INPUT\n",
    "    movie_id - a movie_id \n",
    "    OUTPUT\n",
    "    similar_movies - an array of the most similar movies by title\n",
    "    '''\n",
    "    # find the row of each movie id\n",
    "    movie_idx = np.where(movies['movie_id'] == movie_id)[0][0]\n",
    "    \n",
    "    # find the most similar movie indices - to start I said they need to be the same for all content\n",
    "    similar_idxs = np.where(dot_prod_movies[movie_idx] == np.max(dot_prod_movies[movie_idx]))[0]\n",
    "    \n",
    "    # pull the movie titles based on the indices\n",
    "    similar_movies = np.array(movies.iloc[similar_idxs, ]['movie'])\n",
    "    \n",
    "    return similar_movies\n",
    "    \n",
    "    \n",
    "def get_movie_names(movie_ids):\n",
    "    '''\n",
    "    INPUT\n",
    "    movie_ids - a list of movie_ids\n",
    "    OUTPUT\n",
    "    movies - a list of movie names associated with the movie_ids\n",
    "    \n",
    "    '''\n",
    "    movie_lst = list(movies[movies['movie_id'].isin(movie_ids)]['movie'])\n",
    "   \n",
    "    return movie_lst\n",
    "\n",
    "def make_recs():\n",
    "    '''\n",
    "    INPUT\n",
    "    None\n",
    "    OUTPUT\n",
    "    recs - a dictionary with keys of the user and values of the recommendations\n",
    "    '''\n",
    "    # Create dictionary to return with users and ratings\n",
    "    recs = defaultdict(set)\n",
    "    # How many users for progress bar\n",
    "    n_users = len(users)\n",
    "\n",
    "    \n",
    "    # Create the progressbar\n",
    "    cnter = 0\n",
    "    bar = progressbar.ProgressBar(maxval=n_users+1, widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
    "    bar.start()\n",
    "    \n",
    "    # For each user\n",
    "    for user in users:\n",
    "        \n",
    "        # Update the progress bar\n",
    "        cnter+=1 \n",
    "        bar.update(cnter)\n",
    "\n",
    "        # Pull only the reviews the user has seen\n",
    "        reviews_temp = ranked_reviews[ranked_reviews['user_id'] == user]\n",
    "        movies_temp = np.array(reviews_temp['movie_id'])\n",
    "        movie_names = np.array(get_movie_names(movies_temp))\n",
    "\n",
    "        # Look at each of the movies (highest ranked first), \n",
    "        # pull the movies the user hasn't seen that are most similar\n",
    "        # These will be the recommendations - continue until 10 recs \n",
    "        # or you have depleted the movie list for the user\n",
    "        for movie in movies_temp:\n",
    "            rec_movies = find_similar_movies(movie)\n",
    "            temp_recs = np.setdiff1d(rec_movies, movie_names)\n",
    "            recs[user].update(temp_recs)\n",
    "\n",
    "            # If there are more than \n",
    "            if len(recs[user]) > 9:\n",
    "                break\n",
    "\n",
    "    bar.finish()\n",
    "    \n",
    "    return recs\n",
    "\n",
    "recs = make_recs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b27c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore recommendations\n",
    "users_without_all_recs = []\n",
    "users_with_all_recs = []\n",
    "no_recs = []\n",
    "for user, movie_recs in recs.items():\n",
    "    if len(movie_recs) < 10:\n",
    "        users_without_all_recs.append(user)\n",
    "    if len(movie_recs) > 9:\n",
    "        users_with_all_recs.append(user)\n",
    "    if len(movie_recs) == 0:\n",
    "        no_recs.append(user)\n",
    "\n",
    "\n",
    "# Some characteristics of my content based recommendations\n",
    "print(\"There were {} users without all 10 recommendations we would have liked to have.\".format(len(users_without_all_recs)))\n",
    "print(\"There were {} users with all 10 recommendations we would like them to have.\".format(len(users_with_all_recs)))\n",
    "print(\"There were {} users with no recommendations at all!\".format(len(no_recs)))\n",
    "\n",
    "# Closer look at individual user characteristics\n",
    "user_items = reviews[['user_id', 'movie_id', 'rating']]\n",
    "user_by_movie = user_items.groupby(['user_id', 'movie_id'])['rating'].max().unstack()\n",
    "\n",
    "def movies_watched(user_id):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - the user_id of an individual as int\n",
    "    OUTPUT:\n",
    "    movies - an array of movies the user has watched\n",
    "    '''\n",
    "    movies = user_by_movie.loc[user_id][user_by_movie.loc[user_id].isnull() == False].index.values\n",
    "\n",
    "    return movies\n",
    "\n",
    "\n",
    "movies_watched(189)\n",
    "\n",
    "\n",
    "cnter = 0\n",
    "print(\"Some of the movie lists for users without any recommendations include:\")\n",
    "for user_id in no_recs:\n",
    "    print(user_id)\n",
    "    print(get_movie_names(movies_watched(user_id)))\n",
    "    cnter+=1\n",
    "    if cnter > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0467949c",
   "metadata": {},
   "source": [
    "# SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efb5003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import svd_tests as t\n",
    "%matplotlib inline\n",
    "\n",
    "# Read in the datasets\n",
    "movies = pd.read_csv('data/movies_clean.csv')\n",
    "reviews = pd.read_csv('data/reviews_clean.csv')\n",
    "\n",
    "del movies['Unnamed: 0']\n",
    "del reviews['Unnamed: 0']\n",
    "\n",
    "# Create user-by-item matrix\n",
    "user_items = reviews[['user_id', 'movie_id', 'rating']]\n",
    "user_by_movie = user_items.groupby(['user_id', 'movie_id'])['rating'].max().unstack()\n",
    "\n",
    "user_movie_subset = user_by_movie[[73486, 75314,  68646, 99685]].dropna(axis=0)\n",
    "print(user_movie_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fc2731",
   "metadata": {},
   "outputs": [],
   "source": [
    "u, s, vt = np.linalg.svd(user_movie_subset)\n",
    "s.shape, u.shape, vt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6c0467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the dimensions of u, s, and vt as necessary to use four latent features\n",
    "# update the shape of u and store in u_new\n",
    "u_new = u[:, :len(s)]\n",
    "\n",
    "# update the shape of s and store in s_new\n",
    "s_new = np.zeros((len(s), len(s)))\n",
    "s_new[:len(s), :len(s)] = np.diag(s) \n",
    "\n",
    "# Because we are using 4 latent features and there are only 4 movies, \n",
    "# vt and vt_new are the same\n",
    "vt_new = vt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9ed912",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(np.dot(np.dot(u_new, s_new), vt_new), user_movie_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffa9f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_var = np.sum(s**2)\n",
    "var_exp_comp1_and_comp2 = s[0]**2 + s[1]**2\n",
    "perc_exp = round(var_exp_comp1_and_comp2/total_var*100, 2)\n",
    "print(\"The total variance in the original matrix is {}.\".format(total_var))\n",
    "print(\"Ther percentage of variability captured by the first two components is {}%.\".format(perc_exp))\n",
    "\n",
    "assert np.round(perc_exp, 2) == 98.55, \"Oops!  That doesn't look quite right.  You should have total variability as the sum of all the squared elements in the sigma matrix.  Then just the sum of the squared first two elements is the amount explained by the first two latent features.  Try again.\"\n",
    "print(\"Yup!  That all looks good!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6ef72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the dimensions of u, s, and vt as necessary to use four latent features\n",
    "# update the shape of u and store in u_new\n",
    "k = 2\n",
    "u_2 = u[:, :k]\n",
    "\n",
    "# update the shape of s and store in s_new\n",
    "s_2 = np.zeros((k, k))\n",
    "s_2[:k, :k] = np.diag(s[:k]) \n",
    "\n",
    "# Because we are using 2 latent features, we need to update vt this time\n",
    "vt_2 = vt[:k, :]\n",
    "\n",
    "# Compute the dot product\n",
    "pred_ratings = np.dot(np.dot(u_2, s_2), vt_2)\n",
    "\n",
    "# Compute the squared error for each predicted vs. actual rating\n",
    "sum_square_errs = np.sum(np.sum((user_movie_subset - pred_ratings)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fab33ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line adds one nan value as the very first entry in our matrix\n",
    "user_movie_subset.iloc[0, 0] = np.nan\n",
    "\n",
    "# Try svd with this new matrix\n",
    "u, s, vt = np.linalg.svd(user_movie_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7249b4",
   "metadata": {},
   "source": [
    "# Funk SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f761599a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse\n",
    "import svd_tests as t\n",
    "%matplotlib inline\n",
    "\n",
    "# Read in the datasets\n",
    "movies = pd.read_csv('data/movies_clean.csv')\n",
    "reviews = pd.read_csv('data/reviews_clean.csv')\n",
    "\n",
    "del movies['Unnamed: 0']\n",
    "del reviews['Unnamed: 0']\n",
    "\n",
    "# Create user-by-item matrix\n",
    "user_items = reviews[['user_id', 'movie_id', 'rating', 'timestamp']]\n",
    "user_by_movie = user_items.groupby(['user_id', 'movie_id'])['rating'].max().unstack()\n",
    "\n",
    "# Create data subset\n",
    "user_movie_subset = user_by_movie[[73486, 75314,  68646, 99685]].dropna(axis=0)\n",
    "ratings_mat = np.matrix(user_movie_subset)\n",
    "print(ratings_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a47142f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def FunkSVD(ratings_mat, latent_features=4, learning_rate=0.0001, iters=100):\n",
    "    '''\n",
    "    This function performs matrix factorization using a basic form of FunkSVD with no regularization\n",
    "    \n",
    "    INPUT:\n",
    "    ratings_mat - (numpy array) a matrix with users as rows, movies as columns, and ratings as values\n",
    "    latent_features - (int) the number of latent features used\n",
    "    learning_rate - (float) the learning rate \n",
    "    iters - (int) the number of iterations\n",
    "    \n",
    "    OUTPUT:\n",
    "    user_mat - (numpy array) a user by latent feature matrix\n",
    "    movie_mat - (numpy array) a latent feature by movie matrix\n",
    "    '''\n",
    "    \n",
    "    # Set up useful values to be used through the rest of the function\n",
    "    n_users = ratings_mat.shape[0]\n",
    "    n_movies = ratings_mat.shape[1]\n",
    "    num_ratings = np.count_nonzero(~np.isnan(ratings_mat))\n",
    "    \n",
    "    # initialize the user and movie matrices with random values\n",
    "    user_mat = np.random.rand(n_users, latent_features)\n",
    "    movie_mat = np.random.rand(latent_features, n_movies)\n",
    "    \n",
    "    # initialize sse at 0 for first iteration\n",
    "    sse_accum = 0\n",
    "    \n",
    "    # header for running results\n",
    "    print(\"Optimizaiton Statistics\")\n",
    "    print(\"Iterations | Mean Squared Error \")\n",
    "    \n",
    "    # for each iteration\n",
    "    for iteration in range(iters):\n",
    "\n",
    "        # update our sse\n",
    "        old_sse = sse_accum\n",
    "        sse_accum = 0\n",
    "        \n",
    "        # For each user-movie pair\n",
    "        for i in range(n_users):\n",
    "            for j in range(n_movies):\n",
    "                \n",
    "                # if the rating exists\n",
    "                if ratings_mat[i, j] > 0:\n",
    "                    \n",
    "                    # compute the error as the actual minus the dot product of the user and movie latent features\n",
    "                    diff = ratings_mat[i, j] - np.dot(user_mat[i, :], movie_mat[:, j])\n",
    "                    \n",
    "                    # Keep track of the sum of squared errors for the matrix\n",
    "                    sse_accum += diff**2\n",
    "                    \n",
    "                    # update the values in each matrix in the direction of the gradient\n",
    "                    for k in range(latent_features):\n",
    "                        user_mat[i, k] += learning_rate * (2*diff*movie_mat[k, j])\n",
    "                        movie_mat[k, j] += learning_rate * (2*diff*user_mat[i, k])\n",
    "\n",
    "        # print results for iteration\n",
    "        print(\"%d \\t\\t %f\" % (iteration+1, sse_accum / num_ratings))\n",
    "        \n",
    "    return user_mat, movie_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f4497a",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_mat, movie_mat = FunkSVD(ratings_mat, latent_features=4, learning_rate=0.005, iters=10)\n",
    "\n",
    "print(np.dot(user_mat, movie_mat))\n",
    "print(ratings_mat)\n",
    "\n",
    "user_mat, movie_mat = FunkSVD(ratings_mat, latent_features=4, learning_rate=0.005, iters=250)\n",
    "\n",
    "print(np.dot(user_mat, movie_mat))\n",
    "print(ratings_mat)\n",
    "\n",
    "ratings_mat[0, 0] = np.nan\n",
    "ratings_mat\n",
    "\n",
    "# run SVD on the matrix with the missing value\n",
    "user_mat, movie_mat = FunkSVD(ratings_mat, latent_features=4, learning_rate=0.005, iters=250)\n",
    "\n",
    "preds = np.dot(user_mat, movie_mat)\n",
    "print(\"The predicted value for the missing rating is {}:\".format(preds[0,0]))\n",
    "print()\n",
    "print(\"The actual value for the missing rating is {}:\".format(ratings_mat[0,0]))\n",
    "print()\n",
    "assert np.isnan(preds[0,0]) == False\n",
    "print(\"That's right! You just predicted a rating for a user-movie pair that was never rated!\")\n",
    "print(\"But if you look in the original matrix, this was actually a value of 10. Not bad!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae0ded5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up a matrix of the first 1000 users with movie ratings\n",
    "first_1000_users = np.matrix(user_by_movie.head(1000))\n",
    "\n",
    "# perform funkSVD on the matrix of the top 1000 users\n",
    "user_mat, movie_mat = FunkSVD(first_1000_users, latent_features=4, learning_rate=0.005, iters=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766f2a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# How many actual ratings exist in first_1000_users\n",
    "num_ratings = np.count_nonzero(~np.isnan(first_1000_users))\n",
    "print(\"The number of actual ratings in the first_1000_users is {}.\".format(num_ratings))\n",
    "print()\n",
    "\n",
    "# How many ratings did we make for user-movie pairs that didn't have ratings\n",
    "ratings_for_missing = first_1000_users.shape[0]*first_1000_users.shape[1] - num_ratings\n",
    "print(\"The number of ratings made for user-movie pairs that didn't have ratings is {}\".format(ratings_for_missing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e0cd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your results against the solution\n",
    "assert num_ratings == 10852, \"Oops!  The number of actual ratings doesn't quite look right.\"\n",
    "assert ratings_for_missing == 31234148, \"Oops!  The number of movie-user pairs that you made ratings for that didn't actually have ratings doesn't look right.\"\n",
    "\n",
    "# Make sure you made predictions on all the missing user-movie pairs\n",
    "preds = np.dot(user_mat, movie_mat)\n",
    "assert np.isnan(preds).sum() == 0\n",
    "print(\"Nice job!  Looks like you have predictions made for all the missing user-movie pairs! But I still have one question... How good are they?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8f495b",
   "metadata": {},
   "source": [
    "# How doing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ad78d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Read in the datasets\n",
    "movies = pd.read_csv('data/movies_clean.csv')\n",
    "reviews = pd.read_csv('data/reviews_clean.csv')\n",
    "\n",
    "del movies['Unnamed: 0']\n",
    "del reviews['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b73e998",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test(reviews, order_by, training_size, testing_size):\n",
    "    '''    \n",
    "    INPUT:\n",
    "    reviews - (pandas df) dataframe to split into train and test\n",
    "    order_by - (string) column name to sort by\n",
    "    training_size - (int) number of rows in training set\n",
    "    testing_size - (int) number of columns in the test set\n",
    "    \n",
    "    OUTPUT:\n",
    "    training_df -  (pandas df) dataframe of the training set\n",
    "    validation_df - (pandas df) dataframe of the test set\n",
    "    '''\n",
    "    reviews_new = reviews.sort_values(order_by)\n",
    "    training_df = reviews_new.head(training_size)\n",
    "    validation_df = reviews_new.iloc[training_size:training_size+testing_size]\n",
    "    \n",
    "    return training_df, validation_df\n",
    "\n",
    "\n",
    "# Use our function to create training and test datasets\n",
    "train_df, val_df = create_train_test(reviews, 'date', 8000, 2000)\n",
    "\n",
    "# Make sure the dataframes we are using are the right shape\n",
    "assert train_df.shape[0] == 8000, \"The number of rows doesn't look right in the training dataset.\"\n",
    "assert val_df.shape[0] == 2000, \"The number of rows doesn't look right in the validation dataset\"\n",
    "assert str(train_df.tail(1)['date']).split()[1] == '2013-03-15', \"The last date in the training dataset doesn't look like what we expected.\"\n",
    "assert str(val_df.tail(1)['date']).split()[1] == '2013-03-18', \"The last date in the validation dataset doesn't look like what we expected.\"\n",
    "print(\"Nice job!  Looks like you have written a function that provides training and validation dataframes for you to use in the next steps.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7303a26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FunkSVD(ratings_mat, latent_features=12, learning_rate=0.0001, iters=100):\n",
    "    '''\n",
    "    This function performs matrix factorization using a basic form of FunkSVD with no regularization\n",
    "    \n",
    "    INPUT:\n",
    "    ratings_mat - (numpy array) a matrix with users as rows, movies as columns, and ratings as values\n",
    "    latent_features - (int) the number of latent features used\n",
    "    learning_rate - (float) the learning rate \n",
    "    iters - (int) the number of iterations\n",
    "    \n",
    "    OUTPUT:\n",
    "    user_mat - (numpy array) a user by latent feature matrix\n",
    "    movie_mat - (numpy array) a latent feature by movie matrix\n",
    "    '''\n",
    "    \n",
    "    # Set up useful values to be used through the rest of the function\n",
    "    n_users = ratings_mat.shape[0]\n",
    "    n_movies = ratings_mat.shape[1]\n",
    "    num_ratings = np.count_nonzero(~np.isnan(ratings_mat))\n",
    "    \n",
    "    # initialize the user and movie matrices with random values\n",
    "    user_mat = np.random.rand(n_users, latent_features)\n",
    "    movie_mat = np.random.rand(latent_features, n_movies)\n",
    "    \n",
    "    # initialize sse at 0 for first iteration\n",
    "    sse_accum = 0\n",
    "    \n",
    "    # keep track of iteration and MSE\n",
    "    print(\"Optimizaiton Statistics\")\n",
    "    print(\"Iterations | Mean Squared Error \")\n",
    "    \n",
    "    # for each iteration\n",
    "    for iteration in range(iters):\n",
    "\n",
    "        # update our sse\n",
    "        old_sse = sse_accum\n",
    "        sse_accum = 0\n",
    "        \n",
    "        # For each user-movie pair\n",
    "        for i in range(n_users):\n",
    "            for j in range(n_movies):\n",
    "                \n",
    "                # if the rating exists\n",
    "                if ratings_mat[i, j] > 0:\n",
    "                    \n",
    "                    # compute the error as the actual minus the dot product of the user and movie latent features\n",
    "                    diff = ratings_mat[i, j] - np.dot(user_mat[i, :], movie_mat[:, j])\n",
    "                    \n",
    "                    # Keep track of the sum of squared errors for the matrix\n",
    "                    sse_accum += diff**2\n",
    "                    \n",
    "                    # update the values in each matrix in the direction of the gradient\n",
    "                    for k in range(latent_features):\n",
    "                        user_mat[i, k] += learning_rate * (2*diff*movie_mat[k, j])\n",
    "                        movie_mat[k, j] += learning_rate * (2*diff*user_mat[i, k])\n",
    "\n",
    "        # print results\n",
    "        print(\"%d \\t\\t %f\" % (iteration+1, sse_accum / num_ratings))\n",
    "        \n",
    "    return user_mat, movie_mat\n",
    "\n",
    "# Create user-by-item matrix - nothing to do here\n",
    "train_user_item = train_df[['user_id', 'movie_id', 'rating', 'timestamp']]\n",
    "train_data_df = train_user_item.groupby(['user_id', 'movie_id'])['rating'].max().unstack()\n",
    "train_data_np = np.array(train_data_df)\n",
    "\n",
    "# Fit FunkSVD with the specified hyper parameters to the training data\n",
    "user_mat, movie_mat = FunkSVD(train_data_np, latent_features=15, learning_rate=0.005, iters=250)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6deed2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rating(user_matrix, movie_matrix, user_id, movie_id):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_matrix - user by latent factor matrix\n",
    "    movie_matrix - latent factor by movie matrix\n",
    "    user_id - the user_id from the reviews df\n",
    "    movie_id - the movie_id according the movies df\n",
    "    \n",
    "    OUTPUT:\n",
    "    pred - the predicted rating for user_id-movie_id according to FunkSVD\n",
    "    '''\n",
    "    # Use the training data to create a series of users and movies that matches the ordering in training data\n",
    "    user_ids_series = np.array(train_data_df.index)\n",
    "    movie_ids_series = np.array(train_data_df.columns)\n",
    "    \n",
    "    # User row and Movie Column\n",
    "    user_row = np.where(user_ids_series == user_id)[0][0]\n",
    "    movie_col = np.where(movie_ids_series == movie_id)[0][0]\n",
    "    \n",
    "    # Take dot product of that row and column in U and V to make prediction\n",
    "    pred = np.dot(user_matrix[user_row, :], movie_matrix[:, movie_col])\n",
    "    \n",
    "    return pred\n",
    "\n",
    "# Test your function with the first user-movie in the user-movie matrix (notice this is a nan)\n",
    "pred_val = predict_rating(user_mat, movie_mat, 8, 2844)\n",
    "pred_val\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31fbb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_prediction_summary(user_id, movie_id, prediction):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - the user_id from the reviews df\n",
    "    movie_id - the movie_id according the movies df\n",
    "    prediction - the predicted rating for user_id-movie_id\n",
    "    '''\n",
    "    movie_name = str(movies[movies['movie_id'] == movie_id]['movie']) [5:]\n",
    "    movie_name = movie_name.replace('\\nName: movie, dtype: object', '')\n",
    "    print(\"For user {} we predict a {} rating for the movie {}.\".format(user_id, round(prediction, 2), str(movie_name)))\n",
    "\n",
    "# Test your function the the results of the previous function\n",
    "print_prediction_summary(8, 2844, pred_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200de87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_comparison(val_df, num_preds):\n",
    "    '''\n",
    "    INPUT:\n",
    "    val_df - the validation dataset created in the third cell above\n",
    "    num_preds - (int) the number of rows (going in order) you would like to make predictions for\n",
    "    \n",
    "    OUTPUT:\n",
    "    Nothing returned - print a statement about the prediciton made for each row of val_df from row 0 to num_preds\n",
    "    '''\n",
    "    val_users = np.array(val_df['user_id'])\n",
    "    val_movies = np.array(val_df['movie_id'])\n",
    "    val_ratings = np.array(val_df['rating'])\n",
    "    \n",
    "    \n",
    "    for idx in range(num_preds):\n",
    "        pred = predict_rating(user_mat, movie_mat, val_users[idx], val_movies[idx])\n",
    "        print(\"The actual rating for user {} on movie {} is {}.\\n While the predicted rating is {}.\".format(val_users[idx], val_movies[idx], val_ratings[idx], round(pred))) \n",
    "\n",
    "        \n",
    "# Perform the predicted vs. actual for the first 6 rows.  How does it look?\n",
    "validation_comparison(val_df, 6)\n",
    "\n",
    "# Perform the predicted vs. actual for the first 7 rows.  What happened?\n",
    "validation_comparison(val_df, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f7038a",
   "metadata": {},
   "source": [
    "# The Cold Start Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65aba799",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "\n",
    "# Read in the datasets\n",
    "movies = pd.read_csv('data/movies_clean.csv')\n",
    "reviews = pd.read_csv('data/reviews_clean.csv')\n",
    "\n",
    "del movies['Unnamed: 0']\n",
    "del reviews['Unnamed: 0']\n",
    "\n",
    "def create_train_test(reviews, order_by, training_size, testing_size):\n",
    "    '''    \n",
    "    INPUT:\n",
    "    reviews - (pandas df) dataframe to split into train and test\n",
    "    order_by - (string) column name to sort by\n",
    "    training_size - (int) number of rows in training set\n",
    "    testing_size - (int) number of columns in the test set\n",
    "    \n",
    "    OUTPUT:\n",
    "    training_df -  (pandas df) dataframe of the training set\n",
    "    validation_df - (pandas df) dataframe of the test set\n",
    "    '''\n",
    "    reviews_new = reviews.sort_values(order_by)\n",
    "    training_df = reviews_new.head(training_size)\n",
    "    validation_df = reviews_new.iloc[training_size:training_size+testing_size]\n",
    "    \n",
    "    return training_df, validation_df\n",
    "\n",
    "def predict_rating(user_matrix, movie_matrix, user_id, movie_id):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_matrix - user by latent factor matrix\n",
    "    movie_matrix - latent factor by movie matrix\n",
    "    user_id - the user_id from the reviews df\n",
    "    movie_id - the movie_id according the movies df\n",
    "    \n",
    "    OUTPUT:\n",
    "    pred - the predicted rating for user_id-movie_id according to FunkSVD\n",
    "    '''\n",
    "    # Create series of users and movies in the right order\n",
    "    user_ids_series = np.array(train_data_df.index)\n",
    "    movie_ids_series = np.array(train_data_df.columns)\n",
    "    \n",
    "    # User row and Movie Column\n",
    "    user_row = np.where(user_ids_series == user_id)[0][0]\n",
    "    movie_col = np.where(movie_ids_series == movie_id)[0][0]\n",
    "    \n",
    "    # Take dot product of that row and column in U and V to make prediction\n",
    "    pred = np.dot(user_matrix[user_row, :], movie_matrix[:, movie_col])\n",
    "    \n",
    "    return pred\n",
    "\n",
    "# Use our function to create training and test datasets\n",
    "train_df, val_df = create_train_test(reviews, 'date', 8000, 2000)\n",
    "\n",
    "# Create user-by-item matrix - this will keep track of order of users and movies in u and v\n",
    "train_user_item = train_df[['user_id', 'movie_id', 'rating', 'timestamp']]\n",
    "train_data_df = train_user_item.groupby(['user_id', 'movie_id'])['rating'].max().unstack()\n",
    "train_data_np = np.array(train_data_df)\n",
    "\n",
    "# Read in user and movie matrices\n",
    "user_file = open(\"user_matrix\", 'rb')\n",
    "user_mat = pickle.load(user_file)\n",
    "user_file.close()\n",
    "\n",
    "movie_file = open(\"movie_matrix\", 'rb')\n",
    "movie_mat = pickle.load(movie_file)\n",
    "movie_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8e0c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_comparison(val_df, user_mat=user_mat, movie_mat=movie_mat):\n",
    "    '''\n",
    "    INPUT:\n",
    "    val_df - the validation dataset created in the third cell above\n",
    "    user_mat - U matrix in FunkSVD\n",
    "    movie_mat - V matrix in FunkSVD\n",
    "        \n",
    "    OUTPUT:\n",
    "    rmse - RMSE of how far off each value is from it's predicted value\n",
    "    perc_rated - percent of predictions out of all possible that could be rated\n",
    "    actual_v_pred - a 10 x 10 grid with counts for actual vs predicted values\n",
    "    '''\n",
    "        \n",
    "    val_users = np.array(val_df['user_id'])\n",
    "    val_movies = np.array(val_df['movie_id'])\n",
    "    val_ratings = np.array(val_df['rating'])\n",
    "    \n",
    "    sse = 0\n",
    "    num_rated = 0\n",
    "    preds, acts = [], []\n",
    "    actual_v_pred = np.zeros((10,10))\n",
    "    for idx in range(len(val_users)):\n",
    "        try:\n",
    "            pred = predict_rating(user_mat, movie_mat, val_users[idx], val_movies[idx])\n",
    "            sse += (val_ratings[idx] - pred)**2\n",
    "            num_rated+=1\n",
    "            preds.append(pred)\n",
    "            acts.append(val_ratings[idx])\n",
    "            actual_v_pred[11-int(val_ratings[idx]-1), int(round(pred)-1)]+=1\n",
    "        \n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    rmse = np.sqrt(sse/num_rated)\n",
    "    perc_rated = num_rated/len(val_users)\n",
    "    return rmse, perc_rated, actual_v_pred, preds, acts\n",
    "\n",
    "# How well did we do?\n",
    "rmse, perc_rated, actual_v_pred, preds, acts = validation_comparison(val_df)\n",
    "print(rmse, perc_rated)\n",
    "sns.heatmap(actual_v_pred);\n",
    "plt.xticks(np.arange(10), np.arange(1,11));\n",
    "plt.yticks(np.arange(10), np.arange(1,11));\n",
    "plt.xlabel(\"Predicted Values\");\n",
    "plt.ylabel(\"Actual Values\");\n",
    "plt.title(\"Actual vs. Predicted Values\");\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.hist(acts, normed=True, alpha=.5, label='actual');\n",
    "plt.hist(preds, normed=True, alpha=.5, label='predicted');\n",
    "plt.legend(loc=2, prop={'size': 15});\n",
    "plt.xlabel('Rating');\n",
    "plt.title('Predicted vs. Actual Rating');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0b7ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the above, this can be calculated as follows:\n",
    "print(\"Number not rated {}\".format(int(len(val_df['rating'])*(1-perc_rated))))\n",
    "print(\"Number rated {}.\".format(int(len(val_df['rating'])*perc_rated)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db441134",
   "metadata": {},
   "source": [
    "## Content Based For New Movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c594a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset so movie_content is only using the dummy variables for each genre and the 3 century based year dummy columns\n",
    "movie_content = np.array(movies.iloc[:,4:])\n",
    "\n",
    "# Take the dot product to obtain a movie x movie matrix of similarities\n",
    "dot_prod_movies = movie_content.dot(np.transpose(movie_content))\n",
    "\n",
    "\n",
    "def find_similar_movies(movie_id):\n",
    "    '''\n",
    "    INPUT\n",
    "    movie_id - a movie_id \n",
    "    OUTPUT\n",
    "    similar_movies - an array of the most similar movies by title\n",
    "    '''\n",
    "    # find the row of each movie id\n",
    "    movie_idx = np.where(movies['movie_id'] == movie_id)[0][0]\n",
    "    \n",
    "    # find the most similar movie indices - to start I said they need to be the same for all content\n",
    "    similar_idxs = np.where(dot_prod_movies[movie_idx] == np.max(dot_prod_movies[movie_idx]))[0]\n",
    "    \n",
    "    # pull the movie titles based on the indices\n",
    "    similar_movies = np.array(movies.iloc[similar_idxs, ]['movie'])\n",
    "    \n",
    "    return similar_movies\n",
    "    \n",
    "    \n",
    "def get_movie_names(movie_ids):\n",
    "    '''\n",
    "    INPUT\n",
    "    movie_ids - a list of movie_ids\n",
    "    OUTPUT\n",
    "    movies - a list of movie names associated with the movie_ids\n",
    "    \n",
    "    '''\n",
    "    movie_lst = list(movies[movies['movie_id'].isin(movie_ids)]['movie'])\n",
    "   \n",
    "    return movie_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8afc679",
   "metadata": {},
   "source": [
    "## Rank Based For New Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c28e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ranked_df(movies, reviews):\n",
    "        '''\n",
    "        INPUT\n",
    "        movies - the movies dataframe\n",
    "        reviews - the reviews dataframe\n",
    "        \n",
    "        OUTPUT\n",
    "        ranked_movies - a dataframe with movies that are sorted by highest avg rating, more reviews, \n",
    "                        then time, and must have more than 4 ratings\n",
    "        '''\n",
    "        \n",
    "        # Pull the average ratings and number of ratings for each movie\n",
    "        movie_ratings = reviews.groupby('movie_id')['rating']\n",
    "        avg_ratings = movie_ratings.mean()\n",
    "        num_ratings = movie_ratings.count()\n",
    "        last_rating = pd.DataFrame(reviews.groupby('movie_id').max()['date'])\n",
    "        last_rating.columns = ['last_rating']\n",
    "\n",
    "        # Add Dates\n",
    "        rating_count_df = pd.DataFrame({'avg_rating': avg_ratings, 'num_ratings': num_ratings})\n",
    "        rating_count_df = rating_count_df.join(last_rating)\n",
    "\n",
    "        # merge with the movies dataset\n",
    "        movie_recs = movies.set_index('movie_id').join(rating_count_df)\n",
    "\n",
    "        # sort by top avg rating and number of ratings\n",
    "        ranked_movies = movie_recs.sort_values(['avg_rating', 'num_ratings', 'last_rating'], ascending=False)\n",
    "\n",
    "        # for edge cases - subset the movie list to those with only 5 or more reviews\n",
    "        ranked_movies = ranked_movies[ranked_movies['num_ratings'] > 4]\n",
    "        \n",
    "        return ranked_movies\n",
    "    \n",
    "\n",
    "def popular_recommendations(user_id, n_top, ranked_movies):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - the user_id (str) of the individual you are making recommendations for\n",
    "    n_top - an integer of the number recommendations you want back\n",
    "    ranked_movies - a pandas dataframe of the already ranked movies based on avg rating, count, and time\n",
    "\n",
    "    OUTPUT:\n",
    "    top_movies - a list of the n_top recommended movies by movie title in order best to worst\n",
    "    '''\n",
    "\n",
    "    top_movies = list(ranked_movies['movie'][:n_top])\n",
    "\n",
    "    return top_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5f6a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_recommendations(_id, _id_type='movie', train_data=train_data_df, \n",
    "                         train_df=train_df, movies=movies, rec_num=5, user_mat=user_mat):\n",
    "    '''\n",
    "    INPUT:\n",
    "    _id - either a user or movie id (int)\n",
    "    _id_type - \"movie\" or \"user\" (str)\n",
    "    train_data - dataframe of data as user-movie matrix\n",
    "    train_df - dataframe of training data reviews\n",
    "    movies - movies df\n",
    "    rec_num - number of recommendations to return (int)\n",
    "    user_mat - the U matrix of matrix factorization\n",
    "    movie_mat - the V matrix of matrix factorization\n",
    "    \n",
    "    OUTPUT:\n",
    "    recs - (array) a list or numpy array of recommended movies like the \n",
    "                   given movie, or recs for a user_id given\n",
    "    '''\n",
    "    # if the user is available from the matrix factorization data, \n",
    "    # I will use this and rank movies based on the predicted values\n",
    "    # For use with user indexing\n",
    "    val_users = train_data_df.index\n",
    "    rec_ids = create_ranked_df(movies, train_df)\n",
    "    \n",
    "    if _id_type == 'user':\n",
    "        if _id in train_data.index:\n",
    "            # Get the index of which row the user is in for use in U matrix\n",
    "            idx = np.where(val_users == _id)[0][0]\n",
    "            \n",
    "            # take the dot product of that row and the V matrix\n",
    "            preds = np.dot(user_mat[idx,:],movie_mat)\n",
    "            \n",
    "            # pull the top movies according to the prediction\n",
    "            indices = preds.argsort()[-rec_num:][::-1] #indices\n",
    "            rec_ids = train_data_df.columns[indices]\n",
    "            rec_names = get_movie_names(rec_ids)\n",
    "            \n",
    "        else:\n",
    "            # if we don't have this user, give just top ratings back\n",
    "            rec_names = popular_recommendations(_id, rec_num, ranked_movies)\n",
    "            \n",
    "    # Find similar movies if it is a movie that is passed\n",
    "    else:\n",
    "        rec_ids = find_similar_movies(_id)\n",
    "        rec_names = get_movie_names(rec_ids)\n",
    "    \n",
    "    return rec_ids, rec_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9aeb3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_recommendations(48, 'user')\n",
    "\n",
    "len(set(val_df['user_id']))\n",
    "\n",
    "# Make recommendations\n",
    "user_recs_dict_with_top = dict()\n",
    "for user_id in set(val_df['user_id']):\n",
    "    user_recs_dict_with_top[user_id] = make_recommendations(user_id, 'user')[1]\n",
    "\n",
    "cnter = 0\n",
    "for user, rec in user_recs_dict_with_top.items():\n",
    "    if cnter < 12:\n",
    "        print(\"For user {}, our recommendations are: \\n {}\".format(user, rec))\n",
    "        cnter+=1\n",
    "    else:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
