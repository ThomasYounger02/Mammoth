{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef08657c",
   "metadata": {},
   "source": [
    "# Project Intro\n",
    "【项目实现】： 任何完成项目的过程，都是由以下三步构成的。\n",
    "<img src=\"pics/流程.png\" width=\"40%\">\n",
    "这不是一个线性的过程，而可能出现“代码实现”碰壁后然后折返“分析过程”，再“代码实现”的情形。\n",
    "\n",
    "【知识地图】：前面6关所讲的爬虫原理，在本质上，是一个我们所操作的对象在不断转换的过程。\n",
    "<img src=\"pics/知识地图.png\" width=\"40%\">\n",
    "从Response对象开始，我们就分成了两条路径，一条路径是数据放在HTML里，所以我们用BeautifulSoup库去解析数据和提取数据；另一条，数据作为Json存储起来，所以我们用response.json()方法去解析，然后提取、存储数据。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0afbd60",
   "metadata": {},
   "source": [
    "# 项目实操\n",
    "\n",
    "## 明确目标\n",
    "\n",
    "爬取知乎大v张佳玮的文章“标题”、“摘要”、“链接”，并存储到本地文件。\n",
    "\n",
    "## 分析过程\n",
    "\n",
    "URL：[https://www.zhihu.com/people/zhang-jia-wei/posts?page=1。](https://www.zhihu.com/people/zhang-jia-wei/posts?page=1%E3%80%82)\n",
    "<img src=\"pics/分析1.png\" width=\"40%\">\n",
    "\n",
    "点击右键——检查——Network，选All（而非XHR），然后刷新网页，点进去第0个请求:posts_by_votes，点Preview。\n",
    "<img src=\"pics/分析2.png\" width=\"40%\">\n",
    "发现有文章标题，看来数据是放在HTML里。走的应该是【知识地图】里上面那条路径。\n",
    "\n",
    "去观察一下网页源代码了，我们点回Elements。\n",
    "<img src=\"pics/分析3.png\" width=\"40%\">\n",
    "发现文章标题对应的就是<a>元素里面的文本“大概，他们就是世上活得最明白的人吧。”还可以看到在<a>标签里面，还有属性target=\"_blank\"，和data-za-detail-view-element_name=\"Title\"。\n",
    "\n",
    "按下command+f(windows电脑是ctrl+f)去搜索target。\n",
    "\n",
    "<a标签>的上一个层级是<h2标签>，并且有class=\"ContentItem-title\"。仍然用command+f搜索“ContentItem-title”，发现这个属性可以帮我们精准定位目标数据，可以用。\n",
    "\n",
    "思路：\n",
    "\n",
    "获取数据——用requests库；解析数据——用BeautifulSoup库；提取数据——用BeautifulSoup里的find_all()，翻页的话观察第一页，到最后一页的网址特征，写循环；存储数据——用csv和openpyxl都可以。基本思路就应该是这样。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988de011",
   "metadata": {},
   "source": [
    "## 代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf6d3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "#引入requests和bs\n",
    "url='https://www.zhihu.com/people/zhang-jia-wei/posts/posts_by_votes?page=1'\n",
    "headers={'user-agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36'}\n",
    "#使用headers是一种默认的习惯，默认你已经掌握啦~\n",
    "res=requests.get(url,headers=headers)\n",
    "#用resquest模块发起请求，将响应的结果赋值给变量res。\n",
    "print(res.status_code)\n",
    "#检查状态码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078b1916",
   "metadata": {},
   "source": [
    "发现状态码显示200，表示请求成功，放心了，可以继续往下。\n",
    "\n",
    "## 重新分析\n",
    "\n",
    "打开Network，点开XHR，同时刷新页面，看到出现了很多个请求。\n",
    "<img src=\"pics/分析4.png\" width=\"40%\">\n",
    "看到两个带articles的请求，感觉有戏。点开首个articles看看preview，一层层点开，看到“title：记住就是一切”，猜测这是一个文章标题。\n",
    "<img src=\"pics/分析5.png\" width=\"40%\">\n",
    "在网页里面用command+f(windows电脑用ctrl+f)搜索一下“记住就是一切”，发现搜不到，奇怪。\n",
    "\n",
    "那就看看跟首个articles请求长得很像的另一个articles的请求好啦，仍然看preview，看到title: \"国产航母下水……让我想到李鸿章和北洋舰队\"，仍然在网页里搜一下：\n",
    "<img src=\"pics/分析6.png\" width=\"40%\">\n",
    "观察第1页对第2页的请求，和第2页里对第3页请求的参数区别，是在headers里面的query string parameters里面。\n",
    "\n",
    "发现除了offset都一样，offset代表起始值，limit表示加载的限制数，通过循环我们是可以爬到所有页数的内容了。\n",
    "\n",
    "思路整理：\n",
    "<img src=\"pics/思路.png\" width=\"40%\">\n",
    "### 代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e83659f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "#引入requests\n",
    "headers={'user-agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36'}\n",
    "#封装headers\n",
    "url='https://www.zhihu.com/api/v4/members/zhang-jia-wei/articles?'\n",
    "#写入网址\n",
    "params={\n",
    "    'include':'data[*].comment_count,suggest_edit,is_normal,thumbnail_extra_info,thumbnail,can_comment,comment_permission,admin_closed_comment,content,voteup_count,created,updated,upvoted_followees,voting,review_info,is_labeled,label_info;data[*].author.badge[?(type=best_answerer)].topics',\n",
    "    'offset':'10',\n",
    "    'limit':'20',\n",
    "    'sort_by':'voteups',\n",
    "    }\n",
    "#封装参数\n",
    "res=requests.get(url,headers=headers,params=params)\n",
    "#发送请求，并把响应内容赋值到变量res里面\n",
    "print(res.status_code)\n",
    "#确认请求成功"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c16d46b",
   "metadata": {},
   "source": [
    "根据json数据结构来提取我们想要的文章标题数据。\n",
    "<img src=\"pics/分析7.png\" width=\"40%\">\n",
    "\n",
    "最外层是一个很大的字典，里面有两大元素，data:和paging:，这两大元素又是键值对应的字典形式，data这个键所对应的值是一个列表，里面有10元素，每个元素又是字典形式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5e32da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "#引入requests\n",
    "headers={'user-agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36'}\n",
    "#封装headers\n",
    "url='https://www.zhihu.com/api/v4/members/zhang-jia-wei/articles?'\n",
    "#写入网址\n",
    "params={\n",
    "    'include':'data[*].comment_count,suggest_edit,is_normal,thumbnail_extra_info,thumbnail,can_comment,comment_permission,admin_closed_comment,content,voteup_count,created,updated,upvoted_followees,voting,review_info,is_labeled,label_info;data[*].author.badge[?(type=best_answerer)].topics',\n",
    "    'offset':'10',\n",
    "    'limit':'20',\n",
    "    'sort_by':'voteups',\n",
    "    }\n",
    "#封装参数\n",
    "res=requests.get(url,headers=headers,params=params)\n",
    "#发送请求，并把响应内容赋值到变量res里面\n",
    "print(res.status_code)\n",
    "#确认这个Response对象状态正确    \n",
    "articles=res.json()\n",
    "#用response.json()方法去解析数据，并赋值到变量articles上面，此时的articles是一个\n",
    "print(articles)\n",
    "#打印这个json文件\n",
    "data=articles['data']\n",
    "#取出键为data的值。\n",
    "for i in data:\n",
    "    print(i['title'])\n",
    "    print(i['url'])\n",
    "    print(i['excerpt'])\n",
    "    #遍历列表，拿到的是列表里的每一个元素，这些元素都是字典，再通过键把值取出来"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767e2edd",
   "metadata": {},
   "source": [
    "看看第一页和最后一页请求的参数区别：\n",
    "\n",
    "对比一下，你会发第一页的is_end是显示false，最后一页的is_end是显示true，这个元素可以帮我们结束循环。\n",
    "\n",
    "设置为爬两页数据就停止。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54625c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "headers={'user-agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36'}\n",
    "url='https://www.zhihu.com/api/v4/members/zhang-jia-wei/articles?'\n",
    "articlelist=[]\n",
    "#建立一个空列表，以待写入数据\n",
    "offset=0\n",
    "#设置offset的起始值为0\n",
    "while True:\n",
    "    params={\n",
    "        'include':'data[*].comment_count,suggest_edit,is_normal,thumbnail_extra_info,thumbnail,can_comment,comment_permission,admin_closed_comment,content,voteup_count,created,updated,upvoted_followees,voting,review_info,is_labeled,label_info;data[*].author.badge[?(type=best_answerer)].topics',\n",
    "        'offset':str(offset),\n",
    "        'limit':'20',\n",
    "        'sort_by':'voteups',\n",
    "        }\n",
    "    #封装参数\n",
    "    res=requests.get(url,headers=headers,params=params)\n",
    "    #发送请求，并把响应内容赋值到变量res里面\n",
    "    articles=res.json()\n",
    "    # print(articles)\n",
    "    data=articles['data']\n",
    "    #定位数据\n",
    "    for i in data:\n",
    "        list1=[i['title'],i['url'],i['excerpt']]\n",
    "        #把数据封装成列表\n",
    "        articlelist.append(list1) \n",
    "    offset=offset+20\n",
    "    #在while循环内部，offset的值每次增加20\n",
    "    if offset>40:\n",
    "        break\n",
    "    #如果offset大于40，即爬了两页，就停止\n",
    "    #if articles['paging']['is_end'] == True:\n",
    "    #如果键is_end所对应的值是True，就结束while循环。\n",
    "        #break\n",
    "print(articlelist)\n",
    "#打印看看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fac14bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 接下来就是存储数据。\n",
    "\n",
    "import requests\n",
    "import csv\n",
    "#引用csv。\n",
    "csv_file=open('articles.csv','w',newline='',encoding='utf-8')\n",
    "#调用open()函数打开csv文件，传入参数：文件名“articles.csv”、写入模式“w”、newline=''。\n",
    "writer = csv.writer(csv_file)\n",
    "# 用csv.writer()函数创建一个writer对象。\n",
    "list2=['标题','链接','摘要']\n",
    "#创建一个列表\n",
    "writer.writerow(list2)\n",
    "#调用writer对象的writerow()方法，可以在csv文件里写入一行文字 “标题”和“链接”和\"摘要\"。\n",
    "\n",
    "headers={'user-agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36'}\n",
    "url='https://www.zhihu.com/api/v4/members/zhang-jia-wei/articles?'\n",
    "offset=0\n",
    "#设置offset的起始值为0\n",
    "while True:\n",
    "    params={\n",
    "        'include':'data[*].comment_count,suggest_edit,is_normal,thumbnail_extra_info,thumbnail,can_comment,comment_permission,admin_closed_comment,content,voteup_count,created,updated,upvoted_followees,voting,review_info,is_labeled,label_info;data[*].author.badge[?(type=best_answerer)].topics',\n",
    "        'offset':str(offset),\n",
    "        'limit':'20',\n",
    "        'sort_by':'voteups',\n",
    "        }\n",
    "    #封装参数\n",
    "    res=requests.get(url,headers=headers,params=params)\n",
    "    #发送请求，并把响应内容赋值到变量res里面\n",
    "    articles=res.json()\n",
    "    print(articles)\n",
    "    data=articles['data']\n",
    "    #定位数据\n",
    "    for i in data:\n",
    "        list1=[i['title'],i['url'],i['excerpt']]\n",
    "        #把目标数据封装成一个列表\n",
    "        writer.writerow(list1)\n",
    "        #调用writerow()方法，把列表list1的内容写入\n",
    "    offset=offset+20\n",
    "    #在while循环内部，offset的值每次增加20\n",
    "    if offset > 40:\n",
    "        break\n",
    "csv_file.close()\n",
    "#写入完成后，关闭文件就大功告成\n",
    "print('okay')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08df3cc8",
   "metadata": {},
   "source": [
    "# 展望未来\n",
    "\n",
    "第8关是运用cookies让浏览器记住我们，第9关是用selenium控制浏览器，第10关是让爬虫程序能定时向我们汇报结果，第11-14关是运用协程和scrapy框架来帮我们提速，并且可以爬取海量的数据，第15关是复习。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
